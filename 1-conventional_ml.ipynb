{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    cohen_kappa_score,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharsCountTransformer(BaseEstimator, TransformerMixin):\n",
    "    def get_relevant_chars(self, tweet):\n",
    "        num_hashtags = tweet.count(\"#\")\n",
    "        num_exclamations = tweet.count(\"!\")\n",
    "        num_interrogations = tweet.count(\"?\")\n",
    "        num_at = tweet.count(\"@\")\n",
    "        num_uc = sum(1 for c in tweet if c.isupper())\n",
    "        return [num_hashtags, num_exclamations, num_interrogations, num_at, num_uc]\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        chars = []\n",
    "        for tweet in X:\n",
    "            chars.append(self.get_relevant_chars(tweet))\n",
    "\n",
    "        return np.array(chars)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_score(test_set, predicted_set):\n",
    "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
    "    medium_predicted = np.array(\n",
    "        [prediction[1] for prediction in predicted_set]\n",
    "    )\n",
    "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
    "    inc_test = np.where(test_set == \"incivilidad\", 1.0, 0.0)\n",
    "    odio_test = np.where(test_set == \"odio\", 1.0, 0.0)\n",
    "    normal_test = np.where(test_set == \"normal\", 1.0, 0.0)\n",
    "    auc_high = roc_auc_score(inc_test, high_predicted)\n",
    "    auc_med = roc_auc_score(odio_test, medium_predicted)\n",
    "    auc_low = roc_auc_score(normal_test, low_predicted)\n",
    "    auc_w = (\n",
    "        normal_test.sum() * auc_low\n",
    "        + odio_test.sum() * auc_med\n",
    "        + inc_test.sum() * auc_high\n",
    "    ) / (normal_test.sum() + odio_test.sum() + inc_test.sum())\n",
    "    return auc_w\n",
    "\n",
    "\n",
    "def evaluate(predicted_probabilities, y_test, labels):\n",
    "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
    "    # entregar el arreglo de clases aprendido por el clasificador.\n",
    "    # (que comunmente, es distinto a ['normal', 'odio', 'incivilidad'])\n",
    "    predicted_labels = [\n",
    "        labels[np.argmax(item)] for item in predicted_probabilities\n",
    "    ]\n",
    "\n",
    "    print(\"Matriz de confusión\")\n",
    "    print(\n",
    "        confusion_matrix(\n",
    "            y_test, predicted_labels, labels=[\"normal\", \"odio\", \"incivilidad\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"\\nReporte de clasificación:\\n\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_test, predicted_labels, labels=[\"normal\", \"odio\", \"incivilidad\"]\n",
    "        )\n",
    "    )\n",
    "    # Reorder predicted probabilities array.\n",
    "    labels = labels.tolist()\n",
    "\n",
    "    predicted_probabilities = predicted_probabilities[\n",
    "        :,\n",
    "        [\n",
    "            labels.index(\"normal\"),\n",
    "            labels.index(\"odio\"),\n",
    "            labels.index(\"incivilidad\"),\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
    "    print(\"Métricas:\\n\\nAUC: \", auc, end=\"\\t\")\n",
    "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
    "    print(\"Kappa:\", kappa, end=\"\\t\")\n",
    "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"------------------------------------------------------\\n\")\n",
    "    return np.array([auc, kappa, accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/new/assignment_1/train/train.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.texto,\n",
    "    data.clase,\n",
    "    shuffle=True,\n",
    "    test_size=0.33,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=stopwords\n",
    ")\n",
    "ml_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"features\",\n",
    "            FeatureUnion(\n",
    "                [\n",
    "                    (\"vectorizer\", vectorizer),\n",
    "                    (\"chars_count\", CharsCountTransformer()),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        (\"clf\", RandomForestClassifier(n_jobs=-1,)),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = TweetTokenizer(reduce_len=False,strip_handles=False)\n",
    "def tweet_tokenizer(text):\n",
    "    return t.tokenize(text)\n",
    "\n",
    "def preprocessor(text):\n",
    "    return re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "[CV] END clf__class_weight=balanced, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.3min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.4min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.4min\n",
      "[CV] END clf__class_weight=balanced, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.4min\n",
      "[CV] END clf__class_weight=balanced, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.4min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.4min\n",
      "[CV] END clf__class_weight=None, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.4min\n",
      "[CV] END clf__class_weight=None, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.4min\n",
      "[CV] END clf__class_weight=None, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.4min\n",
      "[CV] END clf__class_weight=balanced, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7fb2a4688af0>; total time= 1.5min\n",
      "[CV] END clf__class_weight=balanced, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7fe768c94a60>; total time= 1.5min\n",
      "[CV] END clf__class_weight=balanced, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7f5ec2d36940>; total time= 1.5min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7fae87747af0>, features__vectorizer__tokenizer=None; total time= 1.1min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7f99a1ea1af0>, features__vectorizer__tokenizer=None; total time= 1.0min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7f84ec838af0>, features__vectorizer__tokenizer=None; total time= 1.1min\n",
      "[CV] END clf__class_weight=None, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7f9207e67af0>; total time= 1.0min\n",
      "[CV] END clf__class_weight=None, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7f6a4f9a7af0>; total time= 1.0min\n",
      "[CV] END clf__class_weight=None, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7fd74c27baf0>; total time= 1.1min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.4min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.4min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.4min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7fe7673fbaf0>; total time= 1.4min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7fb2a2fb0af0>; total time= 1.4min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7f5ec44a1af0>; total time= 1.4min\n",
      "[CV] END clf__class_weight=None, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7fae85f25af0>, features__vectorizer__tokenizer=None; total time=  49.8s\n",
      "[CV] END clf__class_weight=None, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7f99a155daf0>, features__vectorizer__tokenizer=None; total time=  50.2s\n",
      "[CV] END clf__class_weight=None, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7f84eb0faaf0>, features__vectorizer__tokenizer=None; total time=  51.5s\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7f9209364af0>; total time= 1.7min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7f6a4f974af0>; total time= 1.8min\n",
      "[CV] END clf__class_weight=balanced, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7fe76741aaf0>, features__vectorizer__tokenizer=None; total time= 1.4min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7fd74ae31af0>; total time= 1.8min\n",
      "[CV] END clf__class_weight=None, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.1min\n",
      "[CV] END clf__class_weight=balanced, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7fb2a2f9daf0>, features__vectorizer__tokenizer=None; total time= 1.5min\n",
      "[CV] END clf__class_weight=balanced, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7f5ec2c98940>, features__vectorizer__tokenizer=None; total time= 1.4min\n",
      "[CV] END clf__class_weight=None, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.2min\n",
      "[CV] END clf__class_weight=None, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.1min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7fba57ad5940>, features__vectorizer__tokenizer=None; total time= 1.7min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7f87fcb94af0>, features__vectorizer__tokenizer=None; total time= 1.7min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7f4f18649af0>, features__vectorizer__tokenizer=None; total time= 1.7min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time=  58.7s\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.0min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.0min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.3min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.3min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=None; total time= 1.4min\n",
      "[CV] END clf__class_weight=balanced, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7f5ec2fad550>; total time= 1.6min\n",
      "[CV] END clf__class_weight=balanced, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7f84eaf3baf0>; total time= 1.6min\n",
      "[CV] END clf__class_weight=balanced, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7f99a1e94940>; total time= 1.6min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7fd74c1a49d0>, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7fd74c1a4940>; total time= 1.7min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7fae85f3b430>, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7fae85f3b4c0>; total time= 1.8min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7fb2a4724b80>, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7fb2a4724af0>; total time= 1.8min\n",
      "[CV] END clf__class_weight=balanced, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7f9207c00af0>, features__vectorizer__tokenizer=None; total time=  59.3s\n",
      "[CV] END clf__class_weight=balanced, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7f6a4f816af0>, features__vectorizer__tokenizer=None; total time= 1.0min\n",
      "[CV] END clf__class_weight=balanced, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7fe7677dbaf0>, features__vectorizer__tokenizer=None; total time= 1.0min\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7f5ec2edcaf0>, features__vectorizer__tokenizer=None; total time=  41.0s\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7f84eb26baf0>, features__vectorizer__tokenizer=None; total time=  40.4s\n",
      "[CV] END clf__class_weight=balanced_subsample, clf__criterion=gini, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=<function preprocessor at 0x7f99a1ea2940>, features__vectorizer__tokenizer=None; total time=  38.7s\n",
      "[CV] END clf__class_weight=None, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7fba57ac0940>; total time=  57.0s\n",
      "[CV] END clf__class_weight=None, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7f87fb478af0>; total time=  56.4s\n",
      "[CV] END clf__class_weight=None, clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1), features__vectorizer__preprocessor=None, features__vectorizer__tokenizer=<function tweet_tokenizer at 0x7f4f1959da60>; total time=  52.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('features',\n",
       "                                              FeatureUnion(transformer_list=[('vectorizer',\n",
       "                                                                              TfidfVectorizer(stop_words=['de',\n",
       "                                                                                                          'la',\n",
       "                                                                                                          'que',\n",
       "                                                                                                          'el',\n",
       "                                                                                                          'en',\n",
       "                                                                                                          'y',\n",
       "                                                                                                          'a',\n",
       "                                                                                                          'los',\n",
       "                                                                                                          'del',\n",
       "                                                                                                          'se',\n",
       "                                                                                                          'las',\n",
       "                                                                                                          'por',\n",
       "                                                                                                          'un',\n",
       "                                                                                                          'para',\n",
       "                                                                                                          'con',\n",
       "                                                                                                          'no',\n",
       "                                                                                                          'una',\n",
       "                                                                                                          'su',\n",
       "                                                                                                          'al',\n",
       "                                                                                                          'lo',\n",
       "                                                                                                          'como',\n",
       "                                                                                                          'más',\n",
       "                                                                                                          'pero',\n",
       "                                                                                                          'sus',\n",
       "                                                                                                          'le',\n",
       "                                                                                                          'ya',\n",
       "                                                                                                          'o',\n",
       "                                                                                                          'este',\n",
       "                                                                                                          'sí',\n",
       "                                                                                                          'porque', ...])),\n",
       "                                                                             ('chars_count',\n",
       "                                                                              CharsCountTransformer())]...\n",
       "                                        'clf__max_features': ['log2'],\n",
       "                                        'clf__n_estimators': [800, 1000, 1200],\n",
       "                                        'features__vectorizer__lowercase': [True],\n",
       "                                        'features__vectorizer__ngram_range': [(1,\n",
       "                                                                               1)],\n",
       "                                        'features__vectorizer__preprocessor': [<function preprocessor at 0x7fcfbb8b8430>,\n",
       "                                                                               None],\n",
       "                                        'features__vectorizer__tokenizer': [<function tweet_tokenizer at 0x7fcfbb8b84c0>,\n",
       "                                                                            None]},\n",
       "                   scoring='roc_auc_ovr_weighted', verbose=2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"clf__n_estimators\": [800,1000,1200],\n",
    "    \"clf__criterion\":['gini', 'entropy'],\n",
    "    'clf__max_features':['log2'],\n",
    "    'clf__class_weight':[None,'balanced_subsample','balanced'],\n",
    "    'features__vectorizer__lowercase':[True],\n",
    "    'features__vectorizer__ngram_range':[(1,1)],\n",
    "    'features__vectorizer__tokenizer':[tweet_tokenizer, None],\n",
    "    'features__vectorizer__preprocessor':[preprocessor, None]\n",
    "}\n",
    "search = RandomizedSearchCV(\n",
    "    ml_pipeline,\n",
    "    param_grid,\n",
    "    scoring='roc_auc_ovr_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose = 2,\n",
    "    n_iter=20,\n",
    "    cv=3\n",
    ")\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features__vectorizer__tokenizer': None,\n",
       " 'features__vectorizer__preprocessor': None,\n",
       " 'features__vectorizer__ngram_range': (1, 1),\n",
       " 'features__vectorizer__lowercase': True,\n",
       " 'clf__n_estimators': 1000,\n",
       " 'clf__max_features': 'log2',\n",
       " 'clf__criterion': 'entropy',\n",
       " 'clf__class_weight': 'balanced'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'features__vectorizer__ngram_range': (1, 1),\n",
    " 'features__vectorizer__lowercase': True,\n",
    " 'clf__n_estimators': 1200,\n",
    " 'clf__max_features': 'log2',\n",
    " 'clf__criterion': 'entropy',\n",
    " 'clf__class_weight': 'balanced'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_pipeline.set_params(**search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ml_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión\n",
      "[[1018   27  412]\n",
      " [ 204  322  311]\n",
      " [ 159   14 1564]]\n",
      "\n",
      "Reporte de clasificación:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.74      0.70      0.72      1457\n",
      "        odio       0.89      0.38      0.54       837\n",
      " incivilidad       0.68      0.90      0.78      1737\n",
      "\n",
      "    accuracy                           0.72      4031\n",
      "   macro avg       0.77      0.66      0.68      4031\n",
      "weighted avg       0.75      0.72      0.71      4031\n",
      "\n",
      "Métricas:\n",
      "\n",
      "AUC:  0.901\tKappa: 0.544\tAccuracy: 0.72\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_probabilities = search.best_estimator_.predict_proba(X_test)\n",
    "learned_labels = search.best_estimator_.classes_\n",
    "scores = evaluate(predicted_probabilities, y_test, learned_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
