{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    cohen_kappa_score,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharsCountTransformer(BaseEstimator, TransformerMixin):\n",
    "    def get_relevant_chars(self, tweet):\n",
    "        num_hashtags = tweet.count(\"#\")\n",
    "        num_exclamations = tweet.count(\"!\")\n",
    "        num_interrogations = tweet.count(\"?\")\n",
    "        num_at = tweet.count(\"@\")\n",
    "        return [num_hashtags, num_exclamations, num_interrogations, num_at]\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        chars = []\n",
    "        for tweet in X:\n",
    "            chars.append(self.get_relevant_chars(tweet))\n",
    "\n",
    "        return np.array(chars)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_score(test_set, predicted_set):\n",
    "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
    "    medium_predicted = np.array(\n",
    "        [prediction[1] for prediction in predicted_set]\n",
    "    )\n",
    "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
    "    inc_test = np.where(test_set == \"incivilidad\", 1.0, 0.0)\n",
    "    odio_test = np.where(test_set == \"odio\", 1.0, 0.0)\n",
    "    normal_test = np.where(test_set == \"normal\", 1.0, 0.0)\n",
    "    auc_high = roc_auc_score(inc_test, high_predicted)\n",
    "    auc_med = roc_auc_score(odio_test, medium_predicted)\n",
    "    auc_low = roc_auc_score(normal_test, low_predicted)\n",
    "    auc_w = (\n",
    "        normal_test.sum() * auc_low\n",
    "        + odio_test.sum() * auc_med\n",
    "        + inc_test.sum() * auc_high\n",
    "    ) / (normal_test.sum() + odio_test.sum() + inc_test.sum())\n",
    "    return auc_w\n",
    "\n",
    "\n",
    "def evaluate(predicted_probabilities, y_test, labels):\n",
    "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
    "    # entregar el arreglo de clases aprendido por el clasificador.\n",
    "    # (que comunmente, es distinto a ['normal', 'odio', 'incivilidad'])\n",
    "    predicted_labels = [\n",
    "        labels[np.argmax(item)] for item in predicted_probabilities\n",
    "    ]\n",
    "\n",
    "    print(\"Matriz de confusión\")\n",
    "    print(\n",
    "        confusion_matrix(\n",
    "            y_test, predicted_labels, labels=[\"normal\", \"odio\", \"incivilidad\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"\\nReporte de clasificación:\\n\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_test, predicted_labels, labels=[\"normal\", \"odio\", \"incivilidad\"]\n",
    "        )\n",
    "    )\n",
    "    # Reorder predicted probabilities array.\n",
    "    labels = labels.tolist()\n",
    "\n",
    "    predicted_probabilities = predicted_probabilities[\n",
    "        :,\n",
    "        [\n",
    "            labels.index(\"normal\"),\n",
    "            labels.index(\"odio\"),\n",
    "            labels.index(\"incivilidad\"),\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
    "    print(\"Métricas:\\n\\nAUC: \", auc, end=\"\\t\")\n",
    "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
    "    print(\"Kappa:\", kappa, end=\"\\t\")\n",
    "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"------------------------------------------------------\\n\")\n",
    "    return np.array([auc, kappa, accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/new/assignment_1/train/train.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.texto,\n",
    "    data.clase,\n",
    "    shuffle=True,\n",
    "    test_size=0.33,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=stopwords,\n",
    ")\n",
    "ml_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"features\",\n",
    "            FeatureUnion(\n",
    "                [\n",
    "                    (\"vectorizer\", vectorizer),\n",
    "                    (\"chars_count\", CharsCountTransformer()),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        (\"clf\", RandomForestClassifier(n_estimators=800, n_jobs=-1)),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"clf__n_estimators\": [100,300,800,1000],\n",
    "    \"clf__criterion\":['gini', 'entropy', 'log_loss']\n",
    "}\n",
    "search = GridSearchCV(\n",
    "    ml_pipeline,\n",
    "    param_grid,\n",
    "    n_jobs=-1\n",
    ")\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('vectorizer',\n",
       "                                                 TfidfVectorizer(stop_words=['de',\n",
       "                                                                             'la',\n",
       "                                                                             'que',\n",
       "                                                                             'el',\n",
       "                                                                             'en',\n",
       "                                                                             'y',\n",
       "                                                                             'a',\n",
       "                                                                             'los',\n",
       "                                                                             'del',\n",
       "                                                                             'se',\n",
       "                                                                             'las',\n",
       "                                                                             'por',\n",
       "                                                                             'un',\n",
       "                                                                             'para',\n",
       "                                                                             'con',\n",
       "                                                                             'no',\n",
       "                                                                             'una',\n",
       "                                                                             'su',\n",
       "                                                                             'al',\n",
       "                                                                             'lo',\n",
       "                                                                             'como',\n",
       "                                                                             'más',\n",
       "                                                                             'pero',\n",
       "                                                                             'sus',\n",
       "                                                                             'le',\n",
       "                                                                             'ya',\n",
       "                                                                             'o',\n",
       "                                                                             'este',\n",
       "                                                                             'sí',\n",
       "                                                                             'porque', ...])),\n",
       "                                                ('chars_count',\n",
       "                                                 CharsCountTransformer())])),\n",
       "                ('clf', RandomForestClassifier(n_estimators=800, n_jobs=-1))])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión\n",
      "[[1106   63  288]\n",
      " [ 216  424  197]\n",
      " [ 228   26 1483]]\n",
      "\n",
      "Reporte de clasificación:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.71      0.76      0.74      1457\n",
      "        odio       0.83      0.51      0.63       837\n",
      " incivilidad       0.75      0.85      0.80      1737\n",
      "\n",
      "    accuracy                           0.75      4031\n",
      "   macro avg       0.76      0.71      0.72      4031\n",
      "weighted avg       0.75      0.75      0.74      4031\n",
      "\n",
      "Métricas:\n",
      "\n",
      "AUC:  0.897\tKappa: 0.595\tAccuracy: 0.747\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_probabilities = ml_pipeline.predict_proba(X_test)\n",
    "learned_labels = ml_pipeline.classes_\n",
    "scores = evaluate(predicted_probabilities, y_test, learned_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.897, 0.595, 0.747])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
