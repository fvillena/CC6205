{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    cohen_kappa_score,\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharsCountTransformer(BaseEstimator, TransformerMixin):\n",
    "    def get_relevant_chars(self, tweet):\n",
    "        num_hashtags = tweet.count(\"#\")\n",
    "        num_exclamations = tweet.count(\"!\")\n",
    "        num_interrogations = tweet.count(\"?\")\n",
    "        num_at = tweet.count(\"@\")\n",
    "        return [num_hashtags, num_exclamations, num_interrogations, num_at]\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        chars = []\n",
    "        for tweet in X:\n",
    "            chars.append(self.get_relevant_chars(tweet))\n",
    "\n",
    "        return np.array(chars)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc_score(test_set, predicted_set):\n",
    "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
    "    medium_predicted = np.array(\n",
    "        [prediction[1] for prediction in predicted_set]\n",
    "    )\n",
    "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
    "    inc_test = np.where(test_set == \"incivilidad\", 1.0, 0.0)\n",
    "    odio_test = np.where(test_set == \"odio\", 1.0, 0.0)\n",
    "    normal_test = np.where(test_set == \"normal\", 1.0, 0.0)\n",
    "    auc_high = roc_auc_score(inc_test, high_predicted)\n",
    "    auc_med = roc_auc_score(odio_test, medium_predicted)\n",
    "    auc_low = roc_auc_score(normal_test, low_predicted)\n",
    "    auc_w = (\n",
    "        normal_test.sum() * auc_low\n",
    "        + odio_test.sum() * auc_med\n",
    "        + inc_test.sum() * auc_high\n",
    "    ) / (normal_test.sum() + odio_test.sum() + inc_test.sum())\n",
    "    return auc_w\n",
    "\n",
    "\n",
    "def evaluate(predicted_probabilities, y_test, labels):\n",
    "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
    "    # entregar el arreglo de clases aprendido por el clasificador.\n",
    "    # (que comunmente, es distinto a ['normal', 'odio', 'incivilidad'])\n",
    "    predicted_labels = [\n",
    "        labels[np.argmax(item)] for item in predicted_probabilities\n",
    "    ]\n",
    "\n",
    "    print(\"Matriz de confusión\")\n",
    "    print(\n",
    "        confusion_matrix(\n",
    "            y_test, predicted_labels, labels=[\"normal\", \"odio\", \"incivilidad\"]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"\\nReporte de clasificación:\\n\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            y_test, predicted_labels, labels=[\"normal\", \"odio\", \"incivilidad\"]\n",
    "        )\n",
    "    )\n",
    "    # Reorder predicted probabilities array.\n",
    "    labels = labels.tolist()\n",
    "\n",
    "    predicted_probabilities = predicted_probabilities[\n",
    "        :,\n",
    "        [\n",
    "            labels.index(\"normal\"),\n",
    "            labels.index(\"odio\"),\n",
    "            labels.index(\"incivilidad\"),\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
    "    print(\"Métricas:\\n\\nAUC: \", auc, end=\"\\t\")\n",
    "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
    "    print(\"Kappa:\", kappa, end=\"\\t\")\n",
    "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"------------------------------------------------------\\n\")\n",
    "    return np.array([auc, kappa, accuracy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/new/assignment_1/train/train.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.texto,\n",
    "    data.clase,\n",
    "    shuffle=True,\n",
    "    test_size=0.33,\n",
    "    random_state=42,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=stopwords\n",
    ")\n",
    "ml_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"features\",\n",
    "            FeatureUnion(\n",
    "                [\n",
    "                    (\"vectorizer\", vectorizer),\n",
    "                    (\"chars_count\", CharsCountTransformer()),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        (\"clf\", RandomForestClassifier(n_jobs=-1,)),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV 1/3] END clf__criterion=entropy, clf__max_features=sqrt, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.898 total time=  59.3s\n",
      "[CV 3/3] END clf__criterion=entropy, clf__max_features=sqrt, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.882 total time= 1.0min\n",
      "[CV 2/3] END clf__criterion=entropy, clf__max_features=sqrt, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.888 total time= 1.0min\n",
      "[CV 1/3] END clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.894 total time= 1.5min\n",
      "[CV 2/3] END clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.896 total time= 1.5min\n",
      "[CV 3/3] END clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.886 total time= 1.5min\n",
      "[CV 3/3] END clf__criterion=entropy, clf__max_features=sqrt, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.883 total time= 1.5min\n",
      "[CV 1/3] END clf__criterion=entropy, clf__max_features=sqrt, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.898 total time= 1.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END clf__criterion=entropy, clf__max_features=sqrt, clf__n_estimators=1200, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.888 total time= 1.5min\n",
      "[CV 1/3] END clf__criterion=gini, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.894 total time=  58.1s\n",
      "[CV 2/3] END clf__criterion=gini, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.894 total time=  57.5s\n",
      "[CV 3/3] END clf__criterion=gini, clf__max_features=log2, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.885 total time=  58.4s\n",
      "[CV 2/3] END clf__criterion=entropy, clf__max_features=sqrt, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.888 total time= 1.3min\n",
      "[CV 1/3] END clf__criterion=entropy, clf__max_features=sqrt, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.898 total time= 1.3min\n",
      "[CV 3/3] END clf__criterion=entropy, clf__max_features=sqrt, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.882 total time= 1.3min\n",
      "[CV 2/3] END clf__criterion=entropy, clf__max_features=sqrt, clf__n_estimators=1500, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.889 total time= 2.0min\n",
      "[CV 3/3] END clf__criterion=entropy, clf__max_features=sqrt, clf__n_estimators=1500, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.883 total time= 2.0min\n",
      "[CV 1/3] END clf__criterion=entropy, clf__max_features=sqrt, clf__n_estimators=1500, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.898 total time= 2.0min\n",
      "[CV 1/3] END clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.894 total time= 1.4min\n",
      "[CV 2/3] END clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.895 total time= 1.4min\n",
      "[CV 3/3] END clf__criterion=entropy, clf__max_features=log2, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.887 total time= 1.3min\n",
      "[CV 3/3] END clf__criterion=gini, clf__max_features=None, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.868 total time=10.9min\n",
      "[CV 3/3] END clf__criterion=entropy, clf__max_features=None, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.863 total time= 9.5min\n",
      "[CV 2/3] END clf__criterion=entropy, clf__max_features=None, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.873 total time= 9.8min\n",
      "[CV 2/3] END clf__criterion=gini, clf__max_features=None, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.877 total time=11.4min\n",
      "[CV 1/3] END clf__criterion=entropy, clf__max_features=None, clf__n_estimators=800, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.885 total time=10.2min\n",
      "[CV 1/3] END clf__criterion=gini, clf__max_features=None, clf__n_estimators=1000, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.887 total time=11.6min\n",
      "[CV 3/3] END clf__criterion=gini, clf__max_features=None, clf__n_estimators=1500, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.867 total time=11.5min\n",
      "[CV 2/3] END clf__criterion=gini, clf__max_features=None, clf__n_estimators=1500, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.876 total time=11.9min\n",
      "[CV 1/3] END clf__criterion=gini, clf__max_features=None, clf__n_estimators=1500, features__vectorizer__lowercase=True, features__vectorizer__ngram_range=(1, 1);, score=0.888 total time=12.0min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=Pipeline(steps=[('features',\n",
       "                                              FeatureUnion(transformer_list=[('vectorizer',\n",
       "                                                                              TfidfVectorizer(stop_words=['de',\n",
       "                                                                                                          'la',\n",
       "                                                                                                          'que',\n",
       "                                                                                                          'el',\n",
       "                                                                                                          'en',\n",
       "                                                                                                          'y',\n",
       "                                                                                                          'a',\n",
       "                                                                                                          'los',\n",
       "                                                                                                          'del',\n",
       "                                                                                                          'se',\n",
       "                                                                                                          'las',\n",
       "                                                                                                          'por',\n",
       "                                                                                                          'un',\n",
       "                                                                                                          'para',\n",
       "                                                                                                          'con',\n",
       "                                                                                                          'no',\n",
       "                                                                                                          'una',\n",
       "                                                                                                          'su',\n",
       "                                                                                                          'al',\n",
       "                                                                                                          'lo',\n",
       "                                                                                                          'como',\n",
       "                                                                                                          'más',\n",
       "                                                                                                          'pero',\n",
       "                                                                                                          'sus',\n",
       "                                                                                                          'le',\n",
       "                                                                                                          'ya',\n",
       "                                                                                                          'o',\n",
       "                                                                                                          'este',\n",
       "                                                                                                          'sí',\n",
       "                                                                                                          'porque', ...])),\n",
       "                                                                             ('chars_count',\n",
       "                                                                              CharsCountTransformer())])),\n",
       "                                             ('clf',\n",
       "                                              RandomForestClassifier(n_jobs=-1))]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'clf__criterion': ['gini', 'entropy'],\n",
       "                                        'clf__max_features': [None, 'sqrt',\n",
       "                                                              'log2'],\n",
       "                                        'clf__n_estimators': [800, 1000, 1200,\n",
       "                                                              1500],\n",
       "                                        'features__vectorizer__lowercase': [True],\n",
       "                                        'features__vectorizer__ngram_range': [(1,\n",
       "                                                                               1)]},\n",
       "                   scoring='roc_auc_ovr_weighted', verbose=3)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    \"clf__n_estimators\": [800,1000,1200,1500],\n",
    "    \"clf__criterion\":['gini', 'entropy'],\n",
    "    'clf__max_features':[None,'sqrt','log2'],\n",
    "    'clf__class_weight':[None,'balanced_subsample','balanced'],\n",
    "    'features__vectorizer__lowercase':[True],\n",
    "    'features__vectorizer__ngram_range':[(1,1)]\n",
    "}\n",
    "search = RandomizedSearchCV(\n",
    "    ml_pipeline,\n",
    "    param_grid,\n",
    "    scoring='roc_auc_ovr_weighted',\n",
    "    n_jobs=-1,\n",
    "    verbose = 3,\n",
    "    n_iter=10,\n",
    "    cv=3\n",
    ")\n",
    "search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'features__vectorizer__ngram_range': (1, 1),\n",
       " 'features__vectorizer__lowercase': True,\n",
       " 'clf__n_estimators': 1000,\n",
       " 'clf__max_features': 'log2',\n",
       " 'clf__criterion': 'entropy'}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('features',\n",
       "                 FeatureUnion(transformer_list=[('vectorizer',\n",
       "                                                 TfidfVectorizer(stop_words=['de',\n",
       "                                                                             'la',\n",
       "                                                                             'que',\n",
       "                                                                             'el',\n",
       "                                                                             'en',\n",
       "                                                                             'y',\n",
       "                                                                             'a',\n",
       "                                                                             'los',\n",
       "                                                                             'del',\n",
       "                                                                             'se',\n",
       "                                                                             'las',\n",
       "                                                                             'por',\n",
       "                                                                             'un',\n",
       "                                                                             'para',\n",
       "                                                                             'con',\n",
       "                                                                             'no',\n",
       "                                                                             'una',\n",
       "                                                                             'su',\n",
       "                                                                             'al',\n",
       "                                                                             'lo',\n",
       "                                                                             'como',\n",
       "                                                                             'más',\n",
       "                                                                             'pero',\n",
       "                                                                             'sus',\n",
       "                                                                             'le',\n",
       "                                                                             'ya',\n",
       "                                                                             'o',\n",
       "                                                                             'este',\n",
       "                                                                             'sí',\n",
       "                                                                             'porque', ...])),\n",
       "                                                ('chars_count',\n",
       "                                                 CharsCountTransformer())])),\n",
       "                ('clf',\n",
       "                 RandomForestClassifier(criterion='entropy',\n",
       "                                        max_features='log2', n_estimators=1000,\n",
       "                                        n_jobs=-1))])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusión\n",
      "[[1029   29  399]\n",
      " [ 212  333  292]\n",
      " [ 163   15 1559]]\n",
      "\n",
      "Reporte de clasificación:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.73      0.71      0.72      1457\n",
      "        odio       0.88      0.40      0.55       837\n",
      " incivilidad       0.69      0.90      0.78      1737\n",
      "\n",
      "    accuracy                           0.72      4031\n",
      "   macro avg       0.77      0.67      0.68      4031\n",
      "weighted avg       0.75      0.72      0.71      4031\n",
      "\n",
      "Métricas:\n",
      "\n",
      "AUC:  0.9\tKappa: 0.552\tAccuracy: 0.725\n",
      "------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_probabilities = search.best_estimator_.predict_proba(X_test)\n",
    "learned_labels = search.best_estimator_.classes_\n",
    "scores = evaluate(predicted_probabilities, y_test, learned_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
